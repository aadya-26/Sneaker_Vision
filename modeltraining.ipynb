{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933cd96e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TOP 10 POPULAR SNEAKERS CLASSIFIER - GOOGLE COLAB VERSION\n",
    "# 10 Most Iconic Sneaker Models\n",
    "# ============================================================\n",
    "\n",
    "# ===== STEP 1: SETUP KAGGLE AND DOWNLOAD DATASET =====\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: KAGGLE SETUP & DATASET DOWNLOAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Install kaggle\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Upload kaggle.json\n",
    "print(\"\\n‚ö†Ô∏è  UPLOAD YOUR kaggle.json FILE NOW!\")\n",
    "print(\"    Get it from: https://www.kaggle.com/settings -> Create New API Token\\n\")\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup Kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download dataset\n",
    "print(\"\\nüì• Downloading sneakers dataset...\")\n",
    "!kaggle datasets download -d nikolasgegenava/sneakers-classification\n",
    "\n",
    "# Unzip the dataset\n",
    "print(\"\\nüì¶ Unzipping dataset...\")\n",
    "!unzip -o sneakers-classification.zip -d sneakers_data\n",
    "print(\"‚úÖ Dataset extracted to sneakers_data/\\n\")\n",
    "\n",
    "# Show what's inside\n",
    "print(\"üìÇ Contents of sneakers_data:\")\n",
    "!ls sneakers_data/\n",
    "\n",
    "print(\"\\n‚úÖ Download complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a2d43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===== STEP 2: SELECT TOP 10 SNEAKERS & ORGANIZE =====\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: FILTERING TOP 10 SNEAKERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define the top 10 most popular sneakers we want to classify\n",
    "TOP_10_SNEAKERS = [\n",
    "    \"Nike Air Force 1 Low\",\n",
    "    \"Nike Air Jordan 1 High\",\n",
    "    \"Nike Dunk Low\",\n",
    "    \"Adidas Stan Smith\",\n",
    "    \"Adidas Superstar\",\n",
    "    \"Converse Chuck Taylor All-Star High\",\n",
    "    \"Vans Old Skool\",\n",
    "    \"New Balance 550\",\n",
    "    \"Yeezy Boost 350 V2\",\n",
    "    \"Nike Air Max 90\"\n",
    "]\n",
    "\n",
    "print(\"\\nüéØ Selected Top 10 Sneakers:\")\n",
    "for i, sneaker in enumerate(TOP_10_SNEAKERS, 1):\n",
    "    print(f\"{i:2d}. {sneaker}\")\n",
    "\n",
    "# Base dataset folder\n",
    "base_dir = \"sneakers_data/sneakers-dataset/sneakers-dataset\"\n",
    "\n",
    "# First, let's see ALL folders in the dataset\n",
    "print(f\"\\nüìÅ Scanning {base_dir} folder...\")\n",
    "all_folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "print(f\"Found {len(all_folders)} total folders in dataset\")\n",
    "if len(all_folders) > 0:\n",
    "    print(f\"Sample folders: {all_folders[:5]}\")\n",
    "\n",
    "# Find and collect only the top 10 categories\n",
    "categories = []\n",
    "all_images = defaultdict(list)\n",
    "\n",
    "for item in os.listdir(base_dir):\n",
    "    # Only process if it's one of our top 10\n",
    "    if item.replace('_', ' ').title() in TOP_10_SNEAKERS: # Normalize folder names for comparison\n",
    "        item_path = os.path.join(base_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            images = [os.path.join(item_path, f) for f in os.listdir(item_path)\n",
    "                     if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif'))]\n",
    "            if len(images) > 0:\n",
    "                # Map the dataset folder name to our standard sneaker name\n",
    "                standard_name = item.replace('_', ' ').title()\n",
    "                categories.append(standard_name)\n",
    "                all_images[standard_name] = images\n",
    "\n",
    "# Sort categories to match our TOP_10 order\n",
    "categories = [cat for cat in TOP_10_SNEAKERS if cat in categories]\n",
    "\n",
    "print(f\"\\n‚úÖ Found {len(categories)} categories from our top 10 list:\")\n",
    "total_images = 0\n",
    "for i, cat in enumerate(categories, 1):\n",
    "    count = len(all_images[cat])\n",
    "    total_images += count\n",
    "    print(f\"{i:2d}. {cat}: {count} images\")\n",
    "print(f\"\\nTotal images: {total_images}\")\n",
    "\n",
    "if len(categories) == 0:\n",
    "    print(\"\\n‚ùå ERROR: No matching categories found!\")\n",
    "    print(\"\\nLet me show you what folders exist:\")\n",
    "    print(\"All folders in sneakers_data/sneakers-dataset/sneakers-dataset:\")\n",
    "    for folder in all_folders[:20]:\n",
    "        print(f\"  - {folder}\")\n",
    "    print(\"\\nPlease check if folder names match exactly with TOP_10_SNEAKERS list\")\n",
    "    exit()\n",
    "\n",
    "# Split into train/validation (80/20) for each category\n",
    "train_images = {}\n",
    "val_images = {}\n",
    "\n",
    "for category in categories:\n",
    "    images = all_images[category]\n",
    "    if len(images) >= 2:\n",
    "        train, val = train_test_split(images, test_size=0.2, random_state=42)\n",
    "        train_images[category] = train\n",
    "        val_images[category] = val\n",
    "    else:\n",
    "        train_images[category] = images\n",
    "        val_images[category] = []\n",
    "\n",
    "# Create new directory structure\n",
    "train_base = \"dataset/train\"\n",
    "val_base = \"dataset/validation\"\n",
    "\n",
    "for category in categories:\n",
    "    os.makedirs(os.path.join(train_base, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_base, category), exist_ok=True)\n",
    "\n",
    "# Copy files to train\n",
    "print(\"\\nCopying training images...\")\n",
    "for category in categories:\n",
    "    for img in train_images[category]:\n",
    "        shutil.copy(img, os.path.join(train_base, category))\n",
    "\n",
    "# Copy files to validation\n",
    "print(\"Copying validation images...\")\n",
    "for category in categories:\n",
    "    for img in val_images[category]:\n",
    "        shutil.copy(img, os.path.join(val_base, category))\n",
    "\n",
    "train_total = sum(len(train_images[cat]) for cat in categories)\n",
    "val_total = sum(len(val_images[cat]) for cat in categories)\n",
    "\n",
    "print(f\"\\nüìä Dataset Split:\")\n",
    "print(f\"   Train set: {train_total} images\")\n",
    "print(f\"   Validation set: {val_total} images\")\n",
    "print(\"\\nDataset structure created successfully!\\n\")\n",
    "\n",
    "# ===== STEP 3: SETUP MODEL AND TRAINING =====\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: MODEL SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Custom Dataset class\n",
    "class SneakerDataset(Dataset):\n",
    "    def __init__(self, root_dir, processor, categories):\n",
    "        self.processor = processor\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Create label mapping\n",
    "        self.label_map = {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "        # Load all images from each category\n",
    "        for category in categories:\n",
    "            cat_dir = os.path.join(root_dir, category)\n",
    "            if os.path.exists(cat_dir):\n",
    "                for img_name in os.listdir(cat_dir):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                        self.images.append(os.path.join(cat_dir, img_name))\n",
    "                        self.labels.append(self.label_map[category])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = Image.open(self.images[idx]).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "\n",
    "            # Process image\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = inputs['pixel_values'].squeeze()\n",
    "\n",
    "            return {'pixel_values': pixel_values, 'labels': torch.tensor(label)}\n",
    "        except:\n",
    "            # Skip corrupted images\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "# Load processor and model\n",
    "print(\"\\nLoading model...\")\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Create label mappings\n",
    "label2id = {cat: idx for idx, cat in enumerate(categories)}\n",
    "id2label = {idx: cat for idx, cat in enumerate(categories)}\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=len(categories),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Create datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = SneakerDataset(train_base, processor, categories)\n",
    "val_dataset = SneakerDataset(val_base, processor, categories)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16 if torch.cuda.is_available() else 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 5\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Epochs: {num_epochs}\")\n",
    "print(f\"   Learning rate: 2e-5\")\n",
    "print(f\"   Number of classes: {len(categories)}\\n\")\n",
    "\n",
    "# ===== STEP 4: TRAINING =====\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 4: TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "    for batch in progress_bar:\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f'\\nEpoch {epoch+1} Summary:')\n",
    "    print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        print(f'üéØ New best accuracy! Saving model...')\n",
    "        model.save_pretrained(\"./sneakers_top10_best\")\n",
    "        processor.save_pretrained(\"./sneakers_top10_best\")\n",
    "    print()\n",
    "\n",
    "# Save final model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Saving final model...\")\n",
    "model.save_pretrained(\"./sneakers_top10_final\")\n",
    "processor.save_pretrained(\"./sneakers_top10_final\")\n",
    "print(f\"\\n‚úÖ Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(\"‚úÖ Models saved to:\")\n",
    "print(\"   - sneakers_top10_best (highest accuracy)\")\n",
    "print(\"   - sneakers_top10_final (last epoch)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8df53f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL TRAINING ANALYSIS & VISUALIZATION\n",
    "# Run this after training completes\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the trained model\n",
    "print(\"\\nüì¶ Loading trained model...\")\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\"./sneakers_top10_best\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"./sneakers_top10_best\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get class names\n",
    "class_names = [model.config.id2label[i] for i in range(len(model.config.id2label))]\n",
    "print(f\"‚úÖ Model loaded! Classes: {len(class_names)}\")\n",
    "\n",
    "# ===== 1. CONFUSION MATRIX =====\n",
    "print(\"\\n1Ô∏è‚É£ Generating Confusion Matrix...\")\n",
    "\n",
    "# Get predictions on validation set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        \n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[name[:20] for name in class_names],\n",
    "            yticklabels=[name[:20] for name in class_names],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Validation Set', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== 2. PER-CLASS ACCURACY =====\n",
    "print(\"\\n2Ô∏è‚É£ Per-Class Performance...\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "class_correct = np.diag(cm)\n",
    "class_total = cm.sum(axis=1)\n",
    "class_accuracy = (class_correct / class_total) * 100\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Sneaker': class_names,\n",
    "    'Correct': class_correct,\n",
    "    'Total': class_total,\n",
    "    'Accuracy (%)': class_accuracy\n",
    "}).sort_values('Accuracy (%)', ascending=True)\n",
    "\n",
    "# Plot per-class accuracy\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['#FF6B6B' if acc < 70 else '#FFA07A' if acc < 85 else '#4ECDC4' for acc in metrics_df['Accuracy (%)']]\n",
    "bars = plt.barh(range(len(metrics_df)), metrics_df['Accuracy (%)'], color=colors)\n",
    "plt.yticks(range(len(metrics_df)), [name[:30] for name in metrics_df['Sneaker']])\n",
    "plt.xlabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Per-Class Accuracy', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlim(0, 100)\n",
    "plt.axvline(x=80, color='gray', linestyle='--', alpha=0.5, label='60% threshold')\n",
    "plt.legend()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nüìä Detailed Per-Class Metrics:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# ===== 3. CLASSIFICATION REPORT =====\n",
    "print(\"\\n3Ô∏è‚É£ Classification Report...\")\n",
    "print(\"\\n\" + classification_report(all_labels, all_preds, target_names=class_names, digits=3))\n",
    "\n",
    "# ===== 4. TOP CONFUSED PAIRS =====\n",
    "print(\"\\n4Ô∏è‚É£ Most Confused Sneaker Pairs...\")\n",
    "\n",
    "# Find off-diagonal maximums (most confused pairs)\n",
    "np.fill_diagonal(cm, 0)\n",
    "confused_pairs = []\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        if cm[i][j] > 0:\n",
    "            confused_pairs.append({\n",
    "                'True': class_names[i],\n",
    "                'Predicted': class_names[j],\n",
    "                'Count': cm[i][j]\n",
    "            })\n",
    "\n",
    "confused_df = pd.DataFrame(confused_pairs).sort_values('Count', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "x_labels = [f\"{row['True'][:15]}\\n‚Üí {row['Predicted'][:15]}\" for _, row in confused_df.iterrows()]\n",
    "plt.bar(range(len(confused_df)), confused_df['Count'], color='#FF6B6B')\n",
    "plt.xticks(range(len(confused_df)), x_labels, rotation=45, ha='right')\n",
    "plt.ylabel('Number of Misclassifications', fontsize=12)\n",
    "plt.title('Top 10 Most Confused Sneaker Pairs', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop Confused Pairs:\")\n",
    "print(confused_df.to_string(index=False))\n",
    "\n",
    "# ===== 5. CONFIDENCE DISTRIBUTION =====\n",
    "print(\"\\n5Ô∏è‚É£ Model Confidence Distribution...\")\n",
    "\n",
    "# Get confidence scores for all predictions\n",
    "confidences = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Analyzing confidence\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        max_probs = probs.max(dim=-1)[0]\n",
    "        confidences.extend(max_probs.cpu().numpy())\n",
    "\n",
    "confidences = np.array(confidences) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(confidences, bins=50, color='#4ECDC4', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(confidences.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {confidences.mean():.1f}%')\n",
    "plt.axvline(np.median(confidences), color='orange', linestyle='--', linewidth=2, label=f'Median: {np.median(confidences):.1f}%')\n",
    "plt.xlabel('Prediction Confidence (%)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Model Confidence Scores', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"   Mean Confidence: {confidences.mean():.2f}%\")\n",
    "print(f\"   Median Confidence: {np.median(confidences):.2f}%\")\n",
    "print(f\"   Min Confidence: {confidences.min():.2f}%\")\n",
    "print(f\"   Max Confidence: {confidences.max():.2f}%\")\n",
    "print(f\"   Std Deviation: {confidences.std():.2f}%\")\n",
    "\n",
    "# ===== 6. OVERALL SUMMARY =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà OVERALL MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overall_accuracy = (sum(class_correct) / sum(class_total)) * 100\n",
    "print(f\"\\n‚úÖ Overall Validation Accuracy: {overall_accuracy:.2f}%\")\n",
    "print(f\"üìä Total Predictions: {len(all_preds)}\")\n",
    "print(f\"‚úì Correct: {sum(class_correct)}\")\n",
    "print(f\"‚úó Incorrect: {len(all_preds) - sum(class_correct)}\")\n",
    "print(f\"\\nüèÜ Best Performing Class: {metrics_df.iloc[-1]['Sneaker']} ({metrics_df.iloc[-1]['Accuracy (%)']:.2f}%)\")\n",
    "print(f\"‚ö†Ô∏è  Worst Performing Class: {metrics_df.iloc[0]['Sneaker']} ({metrics_df.iloc[0]['Accuracy (%)']:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f521b13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GRADCAM & ATTENTION VISUALIZATION\n",
    "# See what parts of the image the model focuses on!\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from google.colab import files\n",
    "import cv2\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VISUAL EXPLANATION - WHY THIS PREDICTION?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== UPLOAD IMAGE =====\n",
    "print(\"\\nüì§ Upload a sneaker image to analyze:\\n\")\n",
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Load image\n",
    "original_image = Image.open(filename).convert('RGB')\n",
    "print(f\"‚úÖ Loaded: {filename}\")\n",
    "\n",
    "# ===== SETUP MODEL =====\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Ensure output_attentions=True when loading the model to get attention weights\n",
    "model = ViTForImageClassification.from_pretrained(\"./sneakers_top10_best\", output_attentions=True)\n",
    "processor = ViTImageProcessor.from_pretrained(\"./sneakers_top10_best\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Process image\n",
    "inputs = processor(images=original_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ===== 1. GET PREDICTION =====\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    pred_class = logits.argmax(-1).item()\n",
    "    confidence = probs[0][pred_class].item() * 100\n",
    "\n",
    "predicted_sneaker = model.config.id2label[pred_class]\n",
    "print(f\"\\nüéØ Prediction: {predicted_sneaker} ({confidence:.2f}% confidence)\")\n",
    "\n",
    "# ===== 2. ATTENTION ROLLOUT =====\n",
    "print(\"\\nüîç Generating attention visualization...\")\n",
    "\n",
    "# Get attention weights from all layers\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions  # Tuple of attention weights from each layer\n",
    "\n",
    "# Attention rollout - combine attention from all layers\n",
    "def attention_rollout(attentions, discard_ratio=0.9):\n",
    "    # Initialize the accumulated attention matrix with identity for residual connections\n",
    "    # Number of tokens = num_patches + 1 (for CLS token)\n",
    "    num_tokens = attentions[0].size(-1)\n",
    "    result = torch.eye(num_tokens, device=device)\n",
    "\n",
    "    # Process each layer in reverse order (from last layer to first)\n",
    "    for attention_layer in reversed(attentions):\n",
    "        # Average over all heads\n",
    "        # attention_layer.shape: (batch_size, num_heads, sequence_length, sequence_length)\n",
    "        attention_heads_fused = attention_layer.mean(dim=1)[0] # (sequence_length, sequence_length)\n",
    "\n",
    "        # Add identity matrix to account for residual connections\n",
    "        # This makes the attention flow through non-attended parts as well\n",
    "        current_attention = attention_heads_fused + torch.eye(num_tokens, device=device)\n",
    "\n",
    "        # Normalize each row to sum to 1 to represent probability distribution\n",
    "        # Add a small epsilon to the denominator to prevent division by zero\n",
    "        current_attention = current_attention / (current_attention.sum(dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        # Multiply with the accumulated result (matrix multiplication for attention propagation)\n",
    "        result = torch.matmul(current_attention, result)\n",
    "\n",
    "    # Get attention for class token (index 0) to other tokens (index 1 onwards)\n",
    "    mask = result[0, 1:]  # Exclude class token to class token attention\n",
    "\n",
    "    # Apply discard ratio to the final mask if desired\n",
    "    if discard_ratio > 0:\n",
    "        flat_mask = mask.view(-1)\n",
    "        # Only discard if there are enough elements to discard and sum is not already zero\n",
    "        if int(flat_mask.size(-1) * discard_ratio) < flat_mask.size(-1):\n",
    "            _, indices = flat_mask.topk(int(flat_mask.size(-1) * discard_ratio), largest=False)\n",
    "            flat_mask.scatter_(-1, indices, 0)\n",
    "        mask = flat_mask # Update mask to be flat after scattering\n",
    "    \n",
    "    # Re-normalize the final mask if changes were made, ensuring it sums to 1\n",
    "    mask = mask / (mask.sum() + 1e-12)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Get attention mask\n",
    "attention_mask = attention_rollout(attentions).cpu().numpy()\n",
    "\n",
    "# Reshape to 2D (ViT uses 14x14 patches for 224x224 images)\n",
    "num_patches = int(np.sqrt(len(attention_mask)))\n",
    "attention_map = attention_mask.reshape(num_patches, num_patches)\n",
    "\n",
    "# ===== 3. GRADIENT-BASED SALIENCY MAP =====\n",
    "print(\"üî• Generating gradient saliency map...\")\n",
    "\n",
    "# Enable gradients\n",
    "model.zero_grad()\n",
    "inputs_grad = processor(images=original_image, return_tensors=\"pt\").to(device)\n",
    "inputs_grad['pixel_values'].requires_grad = True\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs_grad)\n",
    "target_score = outputs.logits[0, pred_class]\n",
    "\n",
    "# Backward pass\n",
    "target_score.backward()\n",
    "\n",
    "# Get gradients\n",
    "gradients = inputs_grad['pixel_values'].grad.data[0]\n",
    "gradients = gradients.cpu().numpy()\n",
    "\n",
    "# Calculate saliency (absolute value of gradients)\n",
    "saliency = np.abs(gradients).max(axis=0)\n",
    "saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-12) # Add epsilon here too\n",
    "\n",
    "# ===== 4. CREATE VISUALIZATIONS =====\n",
    "print(\"üé® Creating visualizations...\")\n",
    "\n",
    "# Prepare original image as numpy array\n",
    "img_array = np.array(original_image.resize((224, 224)))\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# 1. Original Image\n",
    "ax1 = plt.subplot(2, 4, 1)\n",
    "ax1.imshow(original_image)\n",
    "ax1.set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# 2. Prediction Info\n",
    "ax2 = plt.subplot(2, 4, 2)\n",
    "ax2.axis('off')\n",
    "info_text = f\"Prediction:\\n{predicted_sneaker}\\n\\nConfidence:\\n{confidence:.2f}%\"\n",
    "ax2.text(0.5, 0.5, info_text, ha='center', va='center',\n",
    "         fontsize=16, fontweight='bold',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "# 3. Attention Heatmap (Raw)\n",
    "ax3 = plt.subplot(2, 4, 3)\n",
    "im3 = ax3.imshow(attention_map, cmap='hot', interpolation='bilinear')\n",
    "ax3.set_title('Attention Map (Raw)', fontsize=14, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "plt.colorbar(im3, ax=ax3, fraction=0.046)\n",
    "\n",
    "# 4. Attention Overlay\n",
    "ax4 = plt.subplot(2, 4, 4)\n",
    "attention_resized = cv2.resize(attention_map, (224, 224))\n",
    "attention_resized = (attention_resized - attention_resized.min()) / (attention_resized.max() - attention_resized.min() + 1e-12) # Add epsilon\n",
    "ax4.imshow(img_array)\n",
    "ax4.imshow(attention_resized, cmap='hot', alpha=0.5)\n",
    "ax4.set_title('Attention Overlay', fontsize=14, fontweight='bold')\n",
    "ax4.axis('off')\n",
    "\n",
    "# 5. Gradient Saliency (Raw)\n",
    "ax5 = plt.subplot(2, 4, 5)\n",
    "im5 = ax5.imshow(saliency, cmap='hot')\n",
    "ax5.set_title('Gradient Saliency Map', fontsize=14, fontweight='bold')\n",
    "ax5.axis('off')\n",
    "plt.colorbar(im5, ax=ax5, fraction=0.046)\n",
    "\n",
    "# 6. Gradient Overlay\n",
    "ax6 = plt.subplot(2, 4, 6)\n",
    "ax6.imshow(img_array)\n",
    "ax6.imshow(saliency, cmap='hot', alpha=0.5)\n",
    "ax6.set_title('Gradient Overlay', fontsize=14, fontweight='bold')\n",
    "ax6.axis('off')\n",
    "\n",
    "# 7. Combined Attention + Gradient\n",
    "ax7 = plt.subplot(2, 4, 7)\n",
    "combined = (attention_resized + saliency) / 2\n",
    "im7 = ax7.imshow(combined, cmap='hot')\n",
    "ax7.set_title('Combined Map', fontsize=14, fontweight='bold')\n",
    "ax7.axis('off')\n",
    "plt.colorbar(im7, ax=ax7, fraction=0.046)\n",
    "\n",
    "# 8. Combined Overlay on Original\n",
    "ax8 = plt.subplot(2, 4, 8)\n",
    "ax8.imshow(img_array)\n",
    "ax8.imshow(combined, cmap='hot', alpha=0.6)\n",
    "ax8.set_title('Combined Overlay', fontsize=14, fontweight='bold')\n",
    "ax8.axis('off')\n",
    "\n",
    "plt.suptitle(f'Visual Explanation: Why \"{predicted_sneaker}\"?',\n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== 5. DETAILED ATTENTION ANALYSIS =====\n",
    "print(\"\\nüìä Analyzing attention patterns...\")\n",
    "\n",
    "# Find top attention regions (only if mask is not all zeros/nans)\n",
    "if np.nansum(attention_mask) > 0:\n",
    "    top_patches = np.argsort(attention_mask)[-5:][::-1]\n",
    "    print(f\"\\nTop 5 Most Attended Patches (out of {len(attention_mask)}):\")\n",
    "    for i, patch_idx in enumerate(top_patches, 1):\n",
    "        row = patch_idx // num_patches\n",
    "        col = patch_idx % num_patches\n",
    "        attention_val = attention_mask[patch_idx]\n",
    "        print(f\"{i}. Patch ({row}, {col}): {attention_val:.4f} attention weight\")\n",
    "else:\n",
    "    print(\"\\nCould not determine top attended patches as attention mask is all zeros or NaNs.\")\n",
    "\n",
    "# ===== 6. TOP-K PREDICTIONS WITH ATTENTION =====\n",
    "print(\"\\nüéØ Top 3 Predictions:\")\n",
    "top3_probs, top3_indices = torch.topk(probs[0], min(3, len(probs[0])))\n",
    "\n",
    "for i, (prob, idx) in enumerate(zip(top3_probs, top3_indices), 1):\n",
    "    sneaker_name = model.config.id2label[idx.item()]\n",
    "    prob_percent = prob.item() * 100\n",
    "    bar = \"‚ñà\" * int(prob_percent / 5)\n",
    "    marker = \"üëü \" if i == 1 else \"   \"\n",
    "    print(f\"{marker}{i}. {sneaker_name:<35} {prob_percent:>6.2f}% {bar}\")\n",
    "\n",
    "# ===== INTERPRETATION GUIDE =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìñ HOW TO INTERPRET THESE VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "üî¥ RED/HOT COLORS = High importance/attention\n",
    "üîµ BLUE/COOL COLORS = Low importance/attention\n",
    "\n",
    "1. ATTENTION MAP: Shows which image patches the Vision Transformer\n",
    "   focuses on when making its decision.\n",
    "\n",
    "2. GRADIENT SALIENCY: Shows which pixels, if changed, would most\n",
    "   affect the prediction (based on gradients).\n",
    "\n",
    "3. COMBINED MAP: Merges both attention and gradient information\n",
    "   for a comprehensive view.\n",
    "\n",
    "üí° The model is confident about '{predicted_sneaker}' because it's\n",
    "   focusing on the highlighted regions in these visualizations!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ VISUALIZATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° Run this cell again to analyze another image!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
